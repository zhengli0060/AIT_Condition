indep.matrix[i,1] <- 0
}else{
indep.matrix[i,1] <- 1
}
indep.matrix[i,3] <- result$pvals[1]
Z2_sort <- calculate_probabilities(Z2,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z2, Z2_sort, xis)
# print(result)
if(result$pvals[1] < alpha){
indep.matrix[i,2] <- 0
}else{
indep.matrix[i,2] <- 1
}
indep.matrix[i,4] <- result$pvals[1]
}, error = function(e) {
# 在发生错误时执行的操作（这里是打印错误信息并跳过循环）
cat("Error occurred:", conditionMessage(e), "\n")
return(NULL)  # 返回 NULL 来标识发生了错误
})
# Z1_sort <- calculate_probabilities(Z1,D)
# xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
# result <- Z_validity_test(Y, D, Z1, Z1_sort, xis)
# if(result$pvals[1] < alpha){
#   indep.matrix[i,1] <- 0
# }else{
#   indep.matrix[i,1] <- 1
# }
# indep.matrix[i,3] <- result$pvals[1]
#
# Z2_sort <- calculate_probabilities(Z2,D)
# xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
# result <- Z_validity_test(Y, D, Z2, Z2_sort, xis)
# print(result)
# if(result$pvals[1] < alpha){
#   indep.matrix[i,2] <- 0
# }else{
#   indep.matrix[i,2] <- 1
# }
# indep.matrix[i,4] <- result$pvals[1]
#
#
print(i)
}
indep.matrix.path <- sprintf('compare_result/A2A3_K_test_result_%d.csv', data_samples)
write.csv(indep.matrix, file = indep.matrix.path, row.names = FALSE)
library("rstudioapi")
setwd(dirname(getActiveDocumentContext()$path))
source(file.path("Z_validity_test_package.R"), chdir = TRUE)
calculate_probabilities <- function(Z, D) {
unique_Z <- unique(Z)
probabilities <- numeric(length(unique_Z))
for (i in seq_along(unique_Z)) {
probabilities[i] <- sum(D[Z == unique_Z[i]] == 1) / sum(Z == unique_Z[i])
}
sorted_indices <- order(probabilities)  # 按照概率的大小排序的索引
sorted_unique_Z <- unique_Z[sorted_indices]  # 按照概率大小重新排序的Z值
return(sorted_unique_Z)
}
IV.num <- 2
colnames = c("Z1_indep", "Z2_indep","Z1_Pvalue","Z2_Pvalue")
rownames <- NULL
repeat_time <- 100
indep.matrix <- matrix(NA, nrow = repeat_time,ncol = IV.num*2, byrow = FALSE,dimnames = list(rownames, colnames))
data_samples <- 5000
alpha <- 10/data_samples
for (i in 1:repeat_time) {
# Path of the data set
data_path <- sprintf('compare_data/A2A3_sample_size_%d/%d_%d.csv', data_samples,data_samples, i)
if (!file.exists(data_path)) {
cat(sprintf('\n%s does not exist.\n\n', data_path))
return
}
#
# Data needs to start from 0
data <- read.table(data_path, header = TRUE, sep = ",")
# Z1 <- c(rep(1, 1000), rep(2, 500), rep(3, 500));
# Z2 <- c(rep(1, 1000), rep(2, 500), rep(3, 500));
# D <- rbinom(2000, 1, 1 / 2);
# Y <- D + rnorm(2000);
#
# # 计算 P(D=1 | Z=1)
# p_D1_given_Z1 <- sum(D[Z == 1] == 1) / sum(Z == 1)
#
# # 计算 P(D=1 | Z=0)
# p_D1_given_Z2 <- sum(D[Z == 2] == 1) / sum(Z == 2)
#
# # 计算 P(D=1 | Z=0)
# p_D1_given_Z3 <- sum(D[Z == 3] == 1) / sum(Z == 3)
#
# # 输出结果
# cat("P(D=1 | Z=1):", p_D1_given_Z1, "\n")
# cat("P(D=1 | Z=2):", p_D1_given_Z2, "\n")
# cat("P(D=1 | Z=3):", p_D1_given_Z3, "\n")
Z1 <- data$IV1
Z2 <- data$IV2
D <- data$Treatment
Y <- data$Outcome
tryCatch({
Z1_sort <- calculate_probabilities(Z1,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z1, Z1_sort, xis)
if(result$pvals[1] < alpha){
indep.matrix[i,1] <- 0
}else{
indep.matrix[i,1] <- 1
}
indep.matrix[i,3] <- result$pvals[1]
Z2_sort <- calculate_probabilities(Z2,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z2, Z2_sort, xis)
if(result$pvals[1] < alpha){
indep.matrix[i,2] <- 0
}else{
indep.matrix[i,2] <- 1
}
indep.matrix[i,4] <- result$pvals[1]
}, error = function(e) {
# 在发生错误时执行的操作（这里是打印错误信息并跳过循环）
cat("Error occurred:", conditionMessage(e), "\n")
return(NULL)  # 返回 NULL 来标识发生了错误
})
print(i)
}
indep.matrix.path <- sprintf('compare_result/A2A3_K_test_result_%d.csv', data_samples)
write.csv(indep.matrix, file = indep.matrix.path, row.names = FALSE)
library("rstudioapi")
setwd(dirname(getActiveDocumentContext()$path))
source(file.path("Z_validity_test_package.R"), chdir = TRUE)
calculate_probabilities <- function(Z, D) {
unique_Z <- unique(Z)
probabilities <- numeric(length(unique_Z))
for (i in seq_along(unique_Z)) {
probabilities[i] <- sum(D[Z == unique_Z[i]] == 1) / sum(Z == unique_Z[i])
}
sorted_indices <- order(probabilities)  # 按照概率的大小排序的索引
sorted_unique_Z <- unique_Z[sorted_indices]  # 按照概率大小重新排序的Z值
return(sorted_unique_Z)
}
IV.num <- 2
colnames = c("Z1_indep", "Z2_indep","Z1_Pvalue","Z2_Pvalue")
rownames <- NULL
repeat_time <- 100
indep.matrix <- matrix(NA, nrow = repeat_time,ncol = IV.num*2, byrow = FALSE,dimnames = list(rownames, colnames))
data_samples <- 7000
alpha <- 10/data_samples
for (i in 1:repeat_time) {
# Path of the data set
data_path <- sprintf('compare_data/A2A3_sample_size_%d/%d_%d.csv', data_samples,data_samples, i)
if (!file.exists(data_path)) {
cat(sprintf('\n%s does not exist.\n\n', data_path))
return
}
#
# Data needs to start from 0
data <- read.table(data_path, header = TRUE, sep = ",")
# Z1 <- c(rep(1, 1000), rep(2, 500), rep(3, 500));
# Z2 <- c(rep(1, 1000), rep(2, 500), rep(3, 500));
# D <- rbinom(2000, 1, 1 / 2);
# Y <- D + rnorm(2000);
#
# # 计算 P(D=1 | Z=1)
# p_D1_given_Z1 <- sum(D[Z == 1] == 1) / sum(Z == 1)
#
# # 计算 P(D=1 | Z=0)
# p_D1_given_Z2 <- sum(D[Z == 2] == 1) / sum(Z == 2)
#
# # 计算 P(D=1 | Z=0)
# p_D1_given_Z3 <- sum(D[Z == 3] == 1) / sum(Z == 3)
#
# # 输出结果
# cat("P(D=1 | Z=1):", p_D1_given_Z1, "\n")
# cat("P(D=1 | Z=2):", p_D1_given_Z2, "\n")
# cat("P(D=1 | Z=3):", p_D1_given_Z3, "\n")
Z1 <- data$IV1
Z2 <- data$IV2
D <- data$Treatment
Y <- data$Outcome
tryCatch({
Z1_sort <- calculate_probabilities(Z1,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z1, Z1_sort, xis)
if(result$pvals[1] < alpha){
indep.matrix[i,1] <- 0
}else{
indep.matrix[i,1] <- 1
}
indep.matrix[i,3] <- result$pvals[1]
Z2_sort <- calculate_probabilities(Z2,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z2, Z2_sort, xis)
if(result$pvals[1] < alpha){
indep.matrix[i,2] <- 0
}else{
indep.matrix[i,2] <- 1
}
indep.matrix[i,4] <- result$pvals[1]
}, error = function(e) {
# 在发生错误时执行的操作（这里是打印错误信息并跳过循环）
cat("Error occurred:", conditionMessage(e), "\n")
return(NULL)  # 返回 NULL 来标识发生了错误
})
print(i)
}
indep.matrix.path <- sprintf('compare_result/A2A3_K_test_result_%d.csv', data_samples)
write.csv(indep.matrix, file = indep.matrix.path, row.names = FALSE)
library("rstudioapi")
setwd(dirname(getActiveDocumentContext()$path))
source(file.path("Z_validity_test_package.R"), chdir = TRUE)
calculate_probabilities <- function(Z, D) {
unique_Z <- unique(Z)
probabilities <- numeric(length(unique_Z))
for (i in seq_along(unique_Z)) {
probabilities[i] <- sum(D[Z == unique_Z[i]] == 1) / sum(Z == unique_Z[i])
}
sorted_indices <- order(probabilities)  # 按照概率的大小排序的索引
sorted_unique_Z <- unique_Z[sorted_indices]  # 按照概率大小重新排序的Z值
return(sorted_unique_Z)
}
IV.num <- 2
colnames = c("Z1_indep", "Z2_indep","Z1_Pvalue","Z2_Pvalue")
rownames <- NULL
repeat_time <- 100
indep.matrix <- matrix(NA, nrow = repeat_time,ncol = IV.num*2, byrow = FALSE,dimnames = list(rownames, colnames))
data_samples <- 2000
alpha <- 10/data_samples
for (i in 1:repeat_time) {
# Path of the data set
data_path <- sprintf('compare_data/A2A3_sample_size_%d/%d_%d.csv', data_samples,data_samples, i)
if (!file.exists(data_path)) {
cat(sprintf('\n%s does not exist.\n\n', data_path))
return
}
#
# Data needs to start from 0
data <- read.table(data_path, header = TRUE, sep = ",")
Z1 <- data$IV1
Z2 <- data$IV2
D <- data$Treatment
Y <- data$Outcome
tryCatch({
Z1_sort <- calculate_probabilities(Z1,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z1, Z1_sort, xis)
if(result$pvals[1] < alpha){
indep.matrix[i,1] <- 0
}else{
indep.matrix[i,1] <- 1
}
indep.matrix[i,3] <- result$pvals[1]
Z2_sort <- calculate_probabilities(Z2,D)
xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
result <- Z_validity_test(Y, D, Z2, Z2_sort, xis)
# print(result)
if(result$pvals[1] < alpha){
indep.matrix[i,2] <- 0
}else{
indep.matrix[i,2] <- 1
}
indep.matrix[i,4] <- result$pvals[1]
}, error = function(e) {
# 在发生错误时执行的操作（这里是打印错误信息并跳过循环）
cat("Error occurred:", conditionMessage(e), "\n")
return(NULL)  # 返回 NULL 来标识发生了错误
})
# Z1_sort <- calculate_probabilities(Z1,D)
# xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
# result <- Z_validity_test(Y, D, Z1, Z1_sort, xis)
# if(result$pvals[1] < alpha){
#   indep.matrix[i,1] <- 0
# }else{
#   indep.matrix[i,1] <- 1
# }
# indep.matrix[i,3] <- result$pvals[1]
#
# Z2_sort <- calculate_probabilities(Z2,D)
# xis <- c(sqrt(0.005 * (1 - 0.005)), sqrt(0.05 * (1 - 0.05)), sqrt(0.1 * (1 - 0.1)), 1);
# result <- Z_validity_test(Y, D, Z2, Z2_sort, xis)
# print(result)
# if(result$pvals[1] < alpha){
#   indep.matrix[i,2] <- 0
# }else{
#   indep.matrix[i,2] <- 1
# }
# indep.matrix[i,4] <- result$pvals[1]
#
#
print(i)
}
indep.matrix.path <- sprintf('compare_result/A2A3_K_test_result_%d.csv', data_samples)
write.csv(indep.matrix, file = indep.matrix.path, row.names = FALSE)
rm(list = ls()) # To clear all
# load required packages
packages <- c("boot",
"corpcor",
"MASS",
"parallel",
"pracma",
"knockoff")
lapply(packages,require,character.only=TRUE)
setwd("E:/testability_IV_JMLR/JMLR2023_Burauel2023instrument_validity_test")
# load necessary functions
source('fn_simulate_toy_data.R')
source('fn_2SLS.R')
source('estimate_kappa_bootstrap_wrapper.R')
source('fn_complement.R')
source('fn_complement_knockoff.R')
source('fn_test_instrument_validity.R')
# the following two functions, 'estimate_confounding_via_kernel_smoothing' and
# 'estimate_confounding_sigmas',
# implement two different ways to compute
# the degree of confounding kappa
# these are slightly edited versions of the original code
# provided by Dominik Janzing (see https://webdav.tuebingen.mpg.de/causality/)
# to fit the rest of the instrument test pipeline
# B23 does not contain new results on the estimationg of a degree of confounding!
# see Section H and Section I in B23
source('estimate_confounding_via_kernel_smoothing.R')
source('estimate_confounding_sigmas.R')
# set some parameters for the bootstrap procedure
B = 500 # number of bootstrap draws
ncpus = 1 # number of CPUs (if running bootstrap in parallel on a number of CPUs this should be larger than 1)
# choose kappa method
# can be 'spectral' (see Section H in B23) or 'sigmas' (see Section I in B23)
kappa_method = 'sigmas'
# choose method to compute the synthetic treatment variable
# can be 'standard' (as in Algorithm 2 described in B23) or 'knockoff' (see Appendix K in B23)
synthetic_D_method = 'standard'
# YXD is the dataframe containing
# - Y as first column
# - treatment variable D as last column
# - control covariates in columns 2 to ncol(YXD)-1
# Z is the candidate instrument
sim = fn_simulate_toy_data(sample_size = 1000, d = 10, degree_endog_Z = 0)
View(sim)
View(sim)
sim[["YXD"]]
View(sim$YXD)
fn_simulate_toy_dataA2A3 <- function(sample_size = 1000, d = 5, degree_endog_D = 0.5, degree_relevance_Z = 0.5, degree_endog_Z) {
#draw -error on D- and -U- from a multivariate normal with covariance degree_endog_D
cov_matrix = diag(2)#*error_var
cov_matrix[1,2] = degree_endog_D
cov_matrix[2,1] = degree_endog_D
U_eps = mvrnorm(sample_size, rep(0,2), cov_matrix)
eps = U_eps[,1]
U = U_eps[,2]
# generate covariates
eig_val_range = .5
eigv = runif(d-1,1-eig_val_range,1+eig_val_range)
L = diag(eigv)
V = randortho(d-1)
cov_ee = V %*% L %*% t(V)
X_exo_t = mvrnorm(sample_size,rep(0,d-1),cov_ee)
beta_c = rnorm(d-1,mean = 0, sd = 1)
beta_c = beta_c / sqrt(sum(beta_c^2)) # 'normalize' s.t. norm of parameter vector does not rise with d
X_exo = X_exo_t + as.matrix(U) %*% beta_c
# simulate instrument Z
ez <- rnorm(sample_size)
Z = (ez + degree_endog_Z * U > 0)*1
# create endogenous treatment
beta_dt = rnorm(d-1,mean = 0, sd = 1) # by how much do the X_exo influence D
beta_d = beta_dt / sqrt(sum(beta_dt^2)) # 'normalize' s.t. norm of parameter vector does not rise with d
meanD = mean(X_exo %*% beta_d + eps)
D = (degree_relevance_Z * Z + X_exo %*% beta_d + eps > meanD)*1
X = cbind(X_exo, D)
#generate outcome variable Y
beta_t = rnorm(d-1,mean = 0, sd = 1)
beta = beta_t / sqrt(sum(beta_t^2)) # 'normalize' s.t. norm of parameter vector does not rise with d
# add true treatment effect estimate tau = 1
beta = c(beta,1)
## generate Y
Y = X %*% beta + degree_endog_Z * Z + U
YXD = cbind(Y, X)
return(list('YXD' = YXD, 'Z' = Z))
}
rm(list = ls()) # To clear all
# load required packages
packages <- c("boot",
"corpcor",
"MASS",
"parallel",
"pracma",
"knockoff")
lapply(packages,require,character.only=TRUE)
setwd("E:/testability_IV_JMLR/JMLR2023_Burauel2023instrument_validity_test")
# load necessary functions
source('fn_simulate_toy_data.R')
source('fn_2SLS.R')
source('estimate_kappa_bootstrap_wrapper.R')
source('fn_complement.R')
source('fn_complement_knockoff.R')
source('fn_test_instrument_validity.R')
# the following two functions, 'estimate_confounding_via_kernel_smoothing' and
# 'estimate_confounding_sigmas',
# implement two different ways to compute
# the degree of confounding kappa
# these are slightly edited versions of the original code
# provided by Dominik Janzing (see https://webdav.tuebingen.mpg.de/causality/)
# to fit the rest of the instrument test pipeline
# B23 does not contain new results on the estimationg of a degree of confounding!
# see Section H and Section I in B23
source('estimate_confounding_via_kernel_smoothing.R')
source('estimate_confounding_sigmas.R')
### simulate toy data
# sim = fn_simulate_toy_data(sample_size = 1000, d = 5, degree_endog_D = .5)
# YXD = sim$YXD
# Z = sim$Z
# set some parameters for the bootstrap procedure
B = 500 # number of bootstrap draws
ncpus = 1 # number of CPUs (if running bootstrap in parallel on a number of CPUs this should be larger than 1)
# choose kappa method
# can be 'spectral' (see Section H in B23) or 'sigmas' (see Section I in B23)
kappa_method = 'sigmas'
# choose method to compute the synthetic treatment variable
# can be 'standard' (as in Algorithm 2 described in B23) or 'knockoff' (see Appendix K in B23)
synthetic_D_method = 'standard'
# YXD is the dataframe containing
# - Y as first column
# - treatment variable D as last column
# - control covariates in columns 2 to ncol(YXD)-1
# Z is the invalid candidate instrument
sim_data = fn_simulate_toy_data(sample_size = 1000, d = 10, degree_endog_Z = .5)
IVcompare <- function(sim_data, alpha = 0.05, B = 500, ncpus = 1, kappa_method = 'sigmas', synthetic_D_method = 'standard'){
YXD = sim_data$YXD
Z = sim_data$Z
test_results_invalid_IV = fn_test_instrument_validity(YXD, Z, B,
ncpus,
kappa_method,
synthetic_D_method)
# Get the p-value
pvalue <- test_results_invalid_IV$pseudo_p
# Check validity based on alpha level
if (pvalue < alpha) {
result <- paste0("IV is an invalid IV, p-value = ", pvalue, " (H0: valid IV)")
} else {
result <- paste0("IV is a valid IV, p-value = ", pvalue, " (H0: valid IV)")
}
return(result)  # Optionally return the result
}
# Example usage
sim <- fn_simulate_toy_dataA2A3(sample_size = 1000, d = 3, degree_endog_Z = .5)
fn_simulate_toy_dataA2A3 <- function(sample_size = 1000, d = 5, degree_endog_D = 0.5, degree_relevance_Z = 0.5, degree_endog_Z) {
#draw -error on D- and -U- from a multivariate normal with covariance degree_endog_D
cov_matrix = diag(2)#*error_var
cov_matrix[1,2] = degree_endog_D
cov_matrix[2,1] = degree_endog_D
U_eps = mvrnorm(sample_size, rep(0,2), cov_matrix)
eps = U_eps[,1]
U = U_eps[,2]
# generate covariates
eig_val_range = .5
eigv = runif(d-1,1-eig_val_range,1+eig_val_range)
L = diag(eigv)
V = randortho(d-1)
cov_ee = V %*% L %*% t(V)
X_exo_t = mvrnorm(sample_size,rep(0,d-1),cov_ee)
beta_c = rnorm(d-1,mean = 0, sd = 1)
beta_c = beta_c / sqrt(sum(beta_c^2)) # 'normalize' s.t. norm of parameter vector does not rise with d
X_exo = X_exo_t + as.matrix(U) %*% beta_c
# simulate instrument Z
ez <- rnorm(sample_size)
Z = (ez + degree_endog_Z * U > 0)*1
# create endogenous treatment
beta_dt = rnorm(d-1,mean = 0, sd = 1) # by how much do the X_exo influence D
beta_d = beta_dt / sqrt(sum(beta_dt^2)) # 'normalize' s.t. norm of parameter vector does not rise with d
meanD = mean(X_exo %*% beta_d + eps)
D = (degree_relevance_Z * Z + X_exo %*% beta_d + eps > meanD)*1
X = cbind(X_exo, D)
#generate outcome variable Y
beta_t = rnorm(d-1,mean = 0, sd = 1)
beta = beta_t / sqrt(sum(beta_t^2)) # 'normalize' s.t. norm of parameter vector does not rise with d
# add true treatment effect estimate tau = 1
beta = c(beta,1)
## generate Y
Y = X %*% beta + degree_endog_Z * Z + U
YXD = cbind(Y, X)
return(list('YXD' = YXD, 'Z' = Z))
}
sim <- fn_simulate_toy_dataA2A3(sample_size = 1000, d = 3, degree_endog_Z = .5)
data <- list(YXD = sim$YXD, Z = sim$Z)
result <- IVcompare(data)  # 调用函数并保存返回值
print(result)  # 手动打印结果
YXD = sim_data$YXD
Z = sim_data$Z
setwd("E:\\testability_IV_JMLR\\control_IV\\controlfunctionIV-main\\R")
data_path <- 'T_5000_0.csv'
data <- read.table(data_path, header = TRUE, sep = ",")
Z.id <- "IV2"
D <- scale(data[,"Outcome"])
X <- as.matrix(data[,c("W")])
X <- as.matrix(data[,c("W",Z.id)])
View(X)
library(dplyr)
# load required packages
packages <- c("boot",
"corpcor",
"MASS",
"parallel",
"pracma",
"knockoff",
"dplyr")
YXD <- as.matrix(data %>% select(Treatment, starts_with("W"), Outcome))
View(YXD)
library("rstudioapi")
# Source required R scripts
setwd(dirname(getActiveDocumentContext()$path))
library(readxl)
library(Formula) # 加载Formula包
# source("E:\\testability_IV_JMLR\\control_IV\\controlfunctionIV-main\\R\\pretest.R")
# source("E:\\testability_IV_JMLR\\control_IV\\controlfunctionIV-main\\R\\cf.R")
source("pretest.R")
source("cf.R")
